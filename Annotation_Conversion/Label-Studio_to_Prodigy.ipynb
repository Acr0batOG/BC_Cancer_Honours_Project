{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a514a003-839e-453c-b02b-5495e5b01914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! File saved to ../Prodigy_Annotations/Label-Studio_to_Prodigy/JSON_Reports_Annotated\\9014961272\\602581.jsonl\n",
      "Conversion complete! File saved to ../Prodigy_Annotations/Label-Studio_to_Prodigy/JSON_Reports_Annotated\\9014961272\\649968.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import uuid\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load the English tokenizer from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to create input hash (based on Prodigy's approach)\n",
    "def create_hash(text):\n",
    "    return int(hashlib.md5(text.encode()).hexdigest(), 16) % (2**32)\n",
    "\n",
    "def labelstudio_to_prodigy(ls_data):\n",
    "    prodigy_data = []\n",
    "    \n",
    "    for document in ls_data:\n",
    "        # Extract the text from the document\n",
    "        if \"data\" in document and \"text\" in document[\"data\"]:\n",
    "            # Get the original text\n",
    "            original_text = document[\"data\"][\"text\"]\n",
    "            \n",
    "            # Replace newlines with spaces to create a single continuous text\n",
    "            text = original_text.replace(\"\\n\", \" \")\n",
    "            \n",
    "            # Calculate the mapping between original text positions and new text positions\n",
    "            position_map = {}\n",
    "            new_pos = 0\n",
    "            for old_pos in range(len(original_text)):\n",
    "                if original_text[old_pos] == '\\n':\n",
    "                    # For newlines, don't increment new_pos (as they get replaced with spaces)\n",
    "                    position_map[old_pos] = new_pos\n",
    "                    new_pos += 1  # Add 1 for the space that replaces the newline\n",
    "                else:\n",
    "                    position_map[old_pos] = new_pos\n",
    "                    new_pos += 1\n",
    "            \n",
    "            # Tokenize the text with spaCy\n",
    "            doc = nlp(text)\n",
    "            tokens = []\n",
    "            for i, token in enumerate(doc):\n",
    "                tokens.append({\n",
    "                    \"text\": token.text,\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx + len(token.text),\n",
    "                    \"id\": i,\n",
    "                    \"ws\": token.whitespace_ != \"\"\n",
    "                })\n",
    "            \n",
    "            # Process each annotation in the document\n",
    "            spans = []\n",
    "            \n",
    "            if \"annotations\" in document:\n",
    "                for annotation in document[\"annotations\"]:\n",
    "                    if \"result\" in annotation:\n",
    "                        for result in annotation[\"result\"]:\n",
    "                            if \"value\" in result and \"start\" in result[\"value\"] and \"end\" in result[\"value\"] and \"labels\" in result[\"value\"]:\n",
    "                                # Get the original positions from the annotation\n",
    "                                orig_start = result[\"value\"][\"start\"]\n",
    "                                orig_end = result[\"value\"][\"end\"]\n",
    "                                label = result[\"value\"][\"labels\"][0]  # Taking the first label\n",
    "                                \n",
    "                                # Check if this is one of our known examples\n",
    "                                annotated_text = result[\"value\"][\"text\"]\n",
    "                                \n",
    "                                # For unknown examples, try to match as closely as possible\n",
    "                                # Map the original positions to the new positions\n",
    "                                start = position_map.get(orig_start, orig_start)\n",
    "                                end = position_map.get(orig_end, orig_end)\n",
    "                                \n",
    "                                # Find the token indices\n",
    "                                token_start = None\n",
    "                                token_end = None\n",
    "                                \n",
    "                                for i, token in enumerate(tokens):\n",
    "                                    if token_start is None and token[\"start\"] <= start and token[\"end\"] > start:\n",
    "                                        token_start = i\n",
    "                                    if token[\"start\"] <= end and token[\"end\"] >= end:\n",
    "                                        token_end = i\n",
    "                                        break\n",
    "                                \n",
    "                                # If we couldn't find exact matches, use the nearest tokens\n",
    "                                if token_start is None:\n",
    "                                    for i, token in enumerate(tokens):\n",
    "                                        if token[\"start\"] <= start:\n",
    "                                            token_start = i\n",
    "                                if token_end is None:\n",
    "                                    for i, token in enumerate(tokens):\n",
    "                                        if token[\"end\"] >= end:\n",
    "                                            token_end = i\n",
    "                                            break\n",
    "                                \n",
    "                                spans.append({\n",
    "                                    \"start\": start,\n",
    "                                    \"end\": end,\n",
    "                                    \"token_start\": token_start,\n",
    "                                    \"token_end\": token_end,\n",
    "                                    \"label\": label\n",
    "                                })\n",
    "            \n",
    "            # Generate a unique ID for this annotation\n",
    "            annotator_id = str(uuid.uuid4())\n",
    "            session_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Create the Prodigy entry\n",
    "            prodigy_entry = {\n",
    "                \"text\": text,\n",
    "                \"_input_hash\": create_hash(text),\n",
    "                \"_task_hash\": create_hash(str(spans)),\n",
    "                \"_is_binary\": False,\n",
    "                \"tokens\": tokens,\n",
    "                \"_view_id\": \"ner_manual\",\n",
    "                \"spans\": spans,\n",
    "                \"answer\": \"accept\",\n",
    "                \"_timestamp\": int(time.time()),\n",
    "                \"_annotator_id\": annotator_id,\n",
    "                \"_session_id\": session_id\n",
    "            }\n",
    "            \n",
    "            return prodigy_entry\n",
    "\n",
    "def convert_files(input_dir, output_dir):\n",
    "    \"\"\"Convert Label-Studio JSON files to Prodigy JSONL format.\"\"\"\n",
    "    try:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".json\"):  # Ensure processing only JSON files\n",
    "                    continue\n",
    "                input_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(input_path, input_dir)\n",
    "                relative_path = os.path.splitext(relative_path)[0] \n",
    "                output_path = os.path.join(output_dir, relative_path + \".jsonl\")\n",
    "                \n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                \n",
    "                ls_data = []\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    try:\n",
    "                        ls_data = json.load(file)  # Load full JSON file\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Skipping invalid file {input_path}: {e}\")\n",
    "                        continue\n",
    "                prodigy_json = labelstudio_to_prodigy(ls_data)  # Convert LS JSON to Doccano JSON\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    json.dump(prodigy_json, file, ensure_ascii=False, indent=None, separators=(',', ':'))\n",
    "                \n",
    "                print(f\"Conversion complete! File saved to {output_path}\")\n",
    "    except:\n",
    "        print(\"Error {e} writing to file\")\n",
    "    \n",
    "\n",
    "def main():\n",
    "    input_dir = \"../Label-Studio_Annotations//JSON_Reports_Annotated\"\n",
    "    output_dir = \"../Prodigy_Annotations/Label-Studio_to_Prodigy/JSON_Reports_Annotated\"\n",
    "    convert_files(input_dir, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b68b65-3ed8-47af-972e-42cbf450feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
