{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8c79f68-66e9-4b82-9132-ba165c8eb574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! File saved to ../Prodigy_Annotations/Doccano_to_Prodigy/JSON_Reports_Annotated\\9013540851\\414653.jsonl\n",
      "Conversion complete! File saved to ../Prodigy_Annotations/Doccano_to_Prodigy/JSON_Reports_Annotated\\9013540851\\433023.jsonl\n",
      "Conversion complete! File saved to ../Prodigy_Annotations/Doccano_to_Prodigy/JSON_Reports_Annotated\\9013612611\\311711.jsonl\n",
      "Conversion complete! File saved to ../Prodigy_Annotations/Doccano_to_Prodigy/JSON_Reports_Annotated\\9013612611\\537699.jsonl\n",
      "Conversion complete! File saved to ../Prodigy_Annotations/Doccano_to_Prodigy/JSON_Reports_Annotated\\9013612611\\537705.jsonl\n",
      "Conversion complete! File saved to ../Prodigy_Annotations/Doccano_to_Prodigy/JSON_Reports_Annotated\\9013612611\\569928.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import uuid\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the English tokenizer from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to create input hash (based on Prodigy's approach)\n",
    "def create_hash(text):\n",
    "    return int(hashlib.md5(text.encode()).hexdigest(), 16) % (2**32)\n",
    "\n",
    "def find_token_span(doc, start, end):\n",
    "    \"\"\"\n",
    "    Find the most comprehensive token span for a given character span\n",
    "    with more flexible matching\n",
    "    \"\"\"\n",
    "    token_start = None\n",
    "    token_end = None\n",
    "    \n",
    "    # First, attempt exact matching\n",
    "    for idx, token in enumerate(doc):\n",
    "        if token.idx <= start and (token.idx + len(token.text)) >= end:\n",
    "            token_start = idx\n",
    "            token_end = idx\n",
    "            break\n",
    "    \n",
    "    # If exact match fails, try more flexible matching\n",
    "    if token_start is None:\n",
    "        matching_tokens = []\n",
    "        for idx, token in enumerate(doc):\n",
    "            # Check for overlap\n",
    "            token_start_overlap = max(start, token.idx)\n",
    "            token_end_overlap = min(end, token.idx + len(token.text))\n",
    "            \n",
    "            if token_start_overlap < token_end_overlap:\n",
    "                matching_tokens.append(idx)\n",
    "        \n",
    "        # If we found matching tokens\n",
    "        if matching_tokens:\n",
    "            token_start = min(matching_tokens)\n",
    "            token_end = max(matching_tokens)\n",
    "    \n",
    "    return token_start, token_end\n",
    "\n",
    "def doccano_to_prodigy(doccano_jsonl):\n",
    "    prodigy_data = []\n",
    "    \n",
    "    for entry in doccano_jsonl:\n",
    "        if \"text\" not in entry:\n",
    "            logger.warning(\"Entry missing 'text' field\")\n",
    "            continue\n",
    "        \n",
    "        # Preserve original text exactly\n",
    "        full_text = entry[\"text\"]\n",
    "        full_labels = entry.get(\"label\", [])\n",
    "        \n",
    "        # Tokenize original text with spaCy\n",
    "        doc = nlp(full_text)\n",
    "        \n",
    "        # Create tokens list\n",
    "        tokens = []\n",
    "        for i, token in enumerate(doc):\n",
    "            tokens.append({\n",
    "                \"text\": token.text,\n",
    "                \"start\": token.idx,\n",
    "                \"end\": token.idx + len(token.text),\n",
    "                \"id\": i,\n",
    "                \"ws\": token.whitespace_ != \"\"\n",
    "            })\n",
    "        \n",
    "        # Convert labels to Prodigy-style spans\n",
    "        spans = []\n",
    "        skipped_labels = []\n",
    "        \n",
    "        for start, end, label in full_labels:\n",
    "            # Find the most comprehensive token span\n",
    "            token_start, token_end = find_token_span(doc, start, end)\n",
    "            \n",
    "            # Only add span if we found valid token indices\n",
    "            if token_start is not None and token_end is not None:\n",
    "                spans.append({\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"token_start\": token_start,\n",
    "                    \"token_end\": token_end,\n",
    "                    \"label\": label\n",
    "                })\n",
    "            else:\n",
    "                skipped_labels.append((start, end, label))\n",
    "        \n",
    "        # Log any skipped labels for debugging\n",
    "        if skipped_labels:\n",
    "            logger.warning(f\"Skipped labels: {skipped_labels}\")\n",
    "        \n",
    "        # Generate Prodigy metadata\n",
    "        prodigy_entry = {\n",
    "            \"text\": full_text,\n",
    "            \"_input_hash\": create_hash(full_text),\n",
    "            \"_task_hash\": create_hash(str(spans)),  \n",
    "            \"_is_binary\": False,\n",
    "            \"tokens\": tokens,\n",
    "            \"_view_id\": \"ner_manual\",\n",
    "            \"spans\": spans,\n",
    "            \"answer\": \"accept\",\n",
    "            \"_timestamp\": int(time.time()),  \n",
    "            \"_annotator_id\": str(uuid.uuid4()),  \n",
    "            \"_session_id\": str(uuid.uuid4())  \n",
    "        }\n",
    "        \n",
    "        prodigy_data.append(prodigy_entry)\n",
    "    \n",
    "    return prodigy_data\n",
    "\n",
    "def convert_files(input_dir, output_dir):\n",
    "    \"\"\"Convert Doccano JSONL files to Prodigy JSONL format.\"\"\"\n",
    "    try:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".jsonl\"):\n",
    "                    continue\n",
    "                \n",
    "                input_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(input_path, input_dir)\n",
    "                relative_path = os.path.splitext(relative_path)[0] \n",
    "                output_path = os.path.join(output_dir, relative_path + \".jsonl\")\n",
    "                \n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                \n",
    "                doccano_data = []\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    for line in file:\n",
    "                        try:\n",
    "                            if line.strip():\n",
    "                                doccano_data.append(json.loads(line))\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            logger.error(f\"Skipping invalid line in {input_path}: {line.strip()}\")\n",
    "                \n",
    "                # Convert multiple entries and write to file\n",
    "                prodigy_json_entries = doccano_to_prodigy(doccano_data)\n",
    "                \n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    for entry in prodigy_json_entries:\n",
    "                        file.write(json.dumps(entry) + \"\\n\")\n",
    "                \n",
    "                print(f\"Conversion complete! File saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in conversion: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_dir = \"../Doccano_Annotations/JSON_Reports_Annotated\"\n",
    "    output_dir = \"../Prodigy_Annotations/Doccano_to_Prodigy/JSON_Reports_Annotated\"\n",
    "    convert_files(input_dir, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490956b-7b7a-4eef-9546-142300ab25ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
